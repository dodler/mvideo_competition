{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('X_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UserName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = {}\n",
    "i = 1\n",
    "\n",
    "for user in df['userName'].unique():\n",
    "    t[user] = i\n",
    "    i += 1\n",
    "    \n",
    "for user in t:\n",
    "    df.set_value(df['userName'] == user, 'userName', t[user])\n",
    "    \n",
    "df['userName'] = pd.to_numeric(df['userName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', 4892),\n",
       " ('и', 4425),\n",
       " ('', 3695),\n",
       " ('в', 3279),\n",
       " ('на', 2758),\n",
       " ('-', 1722),\n",
       " ('что', 1616),\n",
       " ('с', 1433),\n",
       " ('но', 1050),\n",
       " ('а', 918),\n",
       " ('как', 808),\n",
       " ('очень', 798),\n",
       " ('по', 767),\n",
       " ('это', 766),\n",
       " ('для', 765),\n",
       " ('при', 721),\n",
       " ('только', 638),\n",
       " ('через', 623),\n",
       " ('все', 607),\n",
       " ('так', 584)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = df[df['reting'] <= 3.5]['comment']\n",
    "splits = list(map(lambda x: x.split(' '), bad))\n",
    "arr = list(itertools.chain.from_iterable(splits))\n",
    "bad = dict(Counter(arr))\n",
    "sorted_bad = sorted(bad.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_bad[: 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 16197),\n",
       " ('не', 13812),\n",
       " ('', 13092),\n",
       " ('в', 11128),\n",
       " ('на', 8793),\n",
       " ('-', 5509),\n",
       " ('с', 5258),\n",
       " ('что', 5182),\n",
       " ('очень', 4425),\n",
       " ('но', 3401),\n",
       " ('для', 3360),\n",
       " ('как', 2720),\n",
       " ('все', 2676),\n",
       " ('по', 2567),\n",
       " ('это', 2524),\n",
       " ('а', 2331),\n",
       " ('у', 2173),\n",
       " ('за', 2157),\n",
       " ('я', 1916),\n",
       " ('так', 1771)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = df[df['reting'] > 3.5]['comment']\n",
    "splits = list(map(lambda x: x.split(' '), good))\n",
    "arr = list(itertools.chain.from_iterable(splits))\n",
    "good = dict(Counter(arr))\n",
    "sorted_good = sorted(good.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_good[: 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('X_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, df.shape[0]):\n",
    "    props = df['property'][i].split(', ')\n",
    "    q = map(lambda x: x.split(': '), props)\n",
    "    props_new = list(q)\n",
    "    \n",
    "    res = []\n",
    "\n",
    "    for j in range(0, len(props_new)):\n",
    "        res.append(props_new[j][0].split('{')[1])\n",
    "    \n",
    "    df.set_value(i, 'property', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Select properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = df[df['reting'] > 3.5]['property']\n",
    "arr = list(itertools.chain.from_iterable(good))\n",
    "good = dict(Counter(arr))\n",
    "\n",
    "props = pd.DataFrame()\n",
    "props['props'] = good.keys()\n",
    "props['count'] = good.values()\n",
    "\n",
    "props_good = list(props[(props['count'] < 500) & (props['count'] > 300)]['props'].astype(int))\n",
    "#props_good = list(props[(props['count'] > 1000)]['props'].astype(int))\n",
    "len(props_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = df[df['reting'] <= 3.5]['property']\n",
    "arr = list(itertools.chain.from_iterable(bad))\n",
    "bad = dict(Counter(arr))\n",
    "\n",
    "props = pd.DataFrame()\n",
    "props['props'] = bad.keys()\n",
    "props['count'] = bad.values()\n",
    "\n",
    "props_bad = list(props[(props['count'] < 900) & (props['count'] > 200)]['props'].astype(int))\n",
    "#props_bad = list(props[(props['count'] > 1000)]['props'].astype(int))\n",
    "len(props_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "props_good_new = set(props_good) - set(props_bad)\n",
    "props_bad_new = set(props_bad) - set(props_good)\n",
    "\n",
    "props_good = props_good_new\n",
    "props_bad = props_bad_new\n",
    "\n",
    "print(len(props_good))\n",
    "print(len(props_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(0, df.shape[0]):\n",
    "    foo = df['property'][j]\n",
    "\n",
    "    count_good = 0\n",
    "    count_bad = 0\n",
    "\n",
    "    for i in range(0, len(foo)):\n",
    "        if int(foo[i]) in props_good:\n",
    "            count_good += 1\n",
    "\n",
    "        if int(foo[i]) in props_bad:\n",
    "            count_bad += 1\n",
    "    \n",
    "#     if count_good > 3:\n",
    "#         df.set_value(j, 'is_good_probs', count_good)#True)\n",
    "#     else:\n",
    "#         df.set_value(j, 'is_good_probs', False)\n",
    "        \n",
    "#     if count_bad > 3:\n",
    "#         df.set_value(j, 'is_bad_probs', count_bad)#True)\n",
    "#     else:\n",
    "#         df.set_value(j, 'is_good_probs', False)\n",
    "        \n",
    "    df.set_value(j, 'is_good_probs', int(count_good))\n",
    "    df.set_value(j, 'is_bad_probs', int(count_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_prop = df[['reting', 'is_good_probs', 'is_bad_probs']].astype(int)\n",
    "#df_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# много свойств в плохом и плохой рейтинг\n",
    "\n",
    "df[(df['is_bad_probs'] > limit) & (df['reting'] < 4)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1561"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# много свойств в плохом и хороший рейтинг\n",
    "\n",
    "df[(df['is_bad_probs'] > limit) & (df['reting'] > 3)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# много свойств в хорошем и хороший рейтинг\n",
    "\n",
    "df[(df['is_good_probs'] > limit) & (df['reting'] > 3)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# много свойств в хорошем и плохой рейтинг\n",
    "\n",
    "df[(df['is_good_probs'] > limit) & (df['reting'] < 4)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, df.shape[0]):\n",
    "    if df['is_bad_probs'][i] > limit:\n",
    "        df['is_good_bool'] = True\n",
    "    else:\n",
    "        df['is_good_bool'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df['reting'].astype(int)\n",
    "X = df.drop(['categoryLevel1Id', 'reting', 'date', 'commentNegative', 'commentPositive'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float or bool.\nDid not expect the data types in fields property, comment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-82a2e05280b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mboost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             train_dmatrix = DMatrix(X, label=training_labels,\n\u001b[0;32m--> 439\u001b[0;31m                                     missing=self.missing)\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         self._Booster = train(xgb_options, train_dmatrix, self.n_estimators,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    253\u001b[0m         data, feature_names, feature_types = _maybe_pandas_data(data,\n\u001b[1;32m    254\u001b[0m                                                                 \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                                                                 feature_types)\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_pandas_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[0;34m(data, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    179\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[1;32m    180\u001b[0m Did not expect the data types in fields \"\"\"\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\nDid not expect the data types in fields property, comment"
     ]
    }
   ],
   "source": [
    "boost = xgb.XGBClassifier(max_depth=15, n_estimators=200)\n",
    "boost.fit(X_train, y_train)\n",
    "y_pred = boost.predict(X_test)\n",
    "print(classification_report(list(y_test), list(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = ExtraTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "est = xgb.XGBRegressor()\n",
    "est.fit(X_train, y_train)\n",
    "y_pred = est.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
