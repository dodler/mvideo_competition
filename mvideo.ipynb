{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('X_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UserName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {}\n",
    "i = 1\n",
    "\n",
    "for user in df['userName'].unique():\n",
    "    t[user] = i\n",
    "    i += 1\n",
    "    \n",
    "for user in t:\n",
    "    df.set_value(df['userName'] == user, 'userName', t[user])\n",
    "    \n",
    "df['userName'] = pd.to_numeric(df['userName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', 4892),\n",
       " ('и', 4425),\n",
       " ('', 3695),\n",
       " ('в', 3279),\n",
       " ('на', 2758),\n",
       " ('-', 1722),\n",
       " ('что', 1616),\n",
       " ('с', 1433),\n",
       " ('но', 1050),\n",
       " ('а', 918),\n",
       " ('как', 808),\n",
       " ('очень', 798),\n",
       " ('по', 767),\n",
       " ('это', 766),\n",
       " ('для', 765),\n",
       " ('при', 721),\n",
       " ('только', 638),\n",
       " ('через', 623),\n",
       " ('все', 607),\n",
       " ('так', 584)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = df[df['reting'] <= 3.5]['comment']\n",
    "splits = list(map(lambda x: x.split(' '), bad))\n",
    "arr = list(itertools.chain.from_iterable(splits))\n",
    "bad = dict(Counter(arr))\n",
    "sorted_bad = sorted(bad.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_bad[: 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 16197),\n",
       " ('не', 13812),\n",
       " ('', 13092),\n",
       " ('в', 11128),\n",
       " ('на', 8793),\n",
       " ('-', 5509),\n",
       " ('с', 5258),\n",
       " ('что', 5182),\n",
       " ('очень', 4425),\n",
       " ('но', 3401),\n",
       " ('для', 3360),\n",
       " ('как', 2720),\n",
       " ('все', 2676),\n",
       " ('по', 2567),\n",
       " ('это', 2524),\n",
       " ('а', 2331),\n",
       " ('у', 2173),\n",
       " ('за', 2157),\n",
       " ('я', 1916),\n",
       " ('так', 1771)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = df[df['reting'] > 3.5]['comment']\n",
    "splits = list(map(lambda x: x.split(' '), good))\n",
    "arr = list(itertools.chain.from_iterable(splits))\n",
    "good = dict(Counter(arr))\n",
    "sorted_good = sorted(good.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_good[: 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, df.shape[0]):\n",
    "    props = df['property'][i].split(', ')\n",
    "    q = map(lambda x: x.split(': '), props)\n",
    "    props_new = list(q)\n",
    "    \n",
    "    res = []\n",
    "\n",
    "    for j in range(0, len(props_new)):\n",
    "        res.append(props_new[j][0].split('{')[1])\n",
    "    \n",
    "    df.set_value(i, 'property', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Select properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = df[df['reting'] > 3.5]['property']\n",
    "arr = list(itertools.chain.from_iterable(good))\n",
    "good = dict(Counter(arr))\n",
    "\n",
    "props = pd.DataFrame()\n",
    "props['props'] = good.keys()\n",
    "props['count'] = good.values()\n",
    "\n",
    "props_good = list(props[(props['count'] < 500) & (props['count'] > 300)]['props'].astype(int))\n",
    "#props_good = list(props[(props['count'] > 1000)]['props'].astype(int))\n",
    "len(props_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = df[df['reting'] <= 3.5]['property']\n",
    "arr = list(itertools.chain.from_iterable(bad))\n",
    "bad = dict(Counter(arr))\n",
    "\n",
    "props = pd.DataFrame()\n",
    "props['props'] = bad.keys()\n",
    "props['count'] = bad.values()\n",
    "\n",
    "props_bad = list(props[(props['count'] < 900) & (props['count'] > 200)]['props'].astype(int))\n",
    "#props_bad = list(props[(props['count'] > 1000)]['props'].astype(int))\n",
    "len(props_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "props_good_new = set(props_good) - set(props_bad)\n",
    "props_bad_new = set(props_bad) - set(props_good)\n",
    "\n",
    "props_good = props_good_new\n",
    "props_bad = props_bad_new\n",
    "\n",
    "print(len(props_good))\n",
    "print(len(props_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0, df.shape[0]):\n",
    "    foo = df['property'][j]\n",
    "\n",
    "    count_good = 0\n",
    "    count_bad = 0\n",
    "\n",
    "    for i in range(0, len(foo)):\n",
    "        if int(foo[i]) in props_good:\n",
    "            count_good += 1\n",
    "\n",
    "        if int(foo[i]) in props_bad:\n",
    "            count_bad += 1\n",
    "    \n",
    "#     if count_good > 3:\n",
    "#         df.set_value(j, 'is_good_probs', count_good)#True)\n",
    "#     else:\n",
    "#         df.set_value(j, 'is_good_probs', False)\n",
    "        \n",
    "#     if count_bad > 3:\n",
    "#         df.set_value(j, 'is_bad_probs', count_bad)#True)\n",
    "#     else:\n",
    "#         df.set_value(j, 'is_good_probs', False)\n",
    "        \n",
    "    df.set_value(j, 'is_good_probs', int(count_good))\n",
    "    df.set_value(j, 'is_bad_probs', int(count_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop = df[['reting', 'is_good_probs', 'is_bad_probs']].astype(int)\n",
    "#df_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# много свойств в плохом и плохой рейтинг\n",
    "\n",
    "df[(df['is_bad_probs'] > limit) & (df['reting'] < 4)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1561"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# много свойств в плохом и хороший рейтинг\n",
    "\n",
    "df[(df['is_bad_probs'] > limit) & (df['reting'] > 3)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# много свойств в хорошем и хороший рейтинг\n",
    "\n",
    "df[(df['is_good_probs'] > limit) & (df['reting'] > 3)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# много свойств в хорошем и плохой рейтинг\n",
    "\n",
    "df[(df['is_good_probs'] > limit) & (df['reting'] < 4)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, df.shape[0]):\n",
    "    if df['is_bad_probs'][i] > limit:\n",
    "        df['is_good_bool'] = True\n",
    "    else:\n",
    "        df['is_good_bool'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, df.shape[0]):\n",
    "    if len(df['comment'][i].split(' ')) > 6:\n",
    "        df['long'] = True\n",
    "    else:\n",
    "        df['long'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import isnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not isnan(df['commentNegative'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not isnan(df['commentNegative'][i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(0, df.shape[0]):\n",
    "    if (not isnan(df['commentNegative'][i])):\n",
    "        if (len(df['commentNegative'][i].split(' ')) > 6):\n",
    "            df['long_negative'] = True\n",
    "        else:\n",
    "            df['long_negative'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['reting'].astype(int)\n",
    "X = df.drop(['categoryLevel1Id', 'reting', 'date', 'commentNegative', 'commentPositive', 'property',\n",
    "             'comment', 'is_good_probs', 'is_bad_probs'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>categoryLevel2Id</th>\n",
       "      <th>brandId</th>\n",
       "      <th>userName</th>\n",
       "      <th>is_good_bool</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20005023</td>\n",
       "      <td>4010201</td>\n",
       "      <td>826</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20020647</td>\n",
       "      <td>4030101</td>\n",
       "      <td>1425</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20020701</td>\n",
       "      <td>4010401</td>\n",
       "      <td>124</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30012256</td>\n",
       "      <td>2030301</td>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30011341</td>\n",
       "      <td>2050201</td>\n",
       "      <td>656</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sku  categoryLevel2Id  brandId  userName  is_good_bool  long\n",
       "0  20005023           4010201      826         1         False  True\n",
       "1  20020647           4030101     1425         2         False  True\n",
       "2  20020701           4010401      124         3         False  True\n",
       "3  30012256           2030301       93         4         False  True\n",
       "4  30011341           2050201      656         5         False  True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.27      0.14      0.18       503\n",
      "          2       0.18      0.07      0.10       289\n",
      "          3       0.15      0.04      0.06       439\n",
      "          4       0.24      0.14      0.18       878\n",
      "          5       0.62      0.85      0.72      3035\n",
      "\n",
      "avg / total       0.46      0.54      0.48      5144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boost = xgb.XGBClassifier(max_depth=15, n_estimators=200)\n",
    "boost.fit(X_train, y_train)\n",
    "y_pred = boost.predict(X_test)\n",
    "print(classification_report(list(y_test), list(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.18      0.19      0.19       503\n",
      "          2       0.10      0.11      0.10       289\n",
      "          3       0.12      0.13      0.12       439\n",
      "          4       0.21      0.22      0.21       878\n",
      "          5       0.64      0.62      0.63      3035\n",
      "\n",
      "avg / total       0.45      0.44      0.44      5144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.21      0.22      0.21       503\n",
      "          2       0.10      0.10      0.10       289\n",
      "          3       0.11      0.10      0.10       439\n",
      "          4       0.21      0.21      0.21       878\n",
      "          5       0.64      0.64      0.64      3035\n",
      "\n",
      "avg / total       0.45      0.45      0.45      5144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = ExtraTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "est = xgb.XGBRegressor()\n",
    "est.fit(X_train, y_train)\n",
    "y_pred = est.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Лян"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['reting'].astype(int)\n",
    "X = df.drop(['categoryLevel1Id', 'reting', 'date', 'commentNegative', 'commentPositive', 'property',\n",
    "             'is_good_probs', 'is_bad_probs'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "cv = CountVectorizer(max_features=100)\n",
    "\n",
    "ft = tfidf.fit_transform(cv.fit_transform(X_train.comment))\n",
    "t_ft = tfidf.fit_transform(cv.fit_transform(X_test.comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ft.toarray()\n",
    "X_test = t_ft.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.393086\ttest-merror:0.413103\n",
      "[25]\ttrain-merror:0.384851\ttest-merror:0.408243\n",
      "[50]\ttrain-merror:0.372115\ttest-merror:0.409409\n",
      "[75]\ttrain-merror:0.356794\ttest-merror:0.411936\n",
      "[100]\ttrain-merror:0.339845\ttest-merror:0.412131\n",
      "[125]\ttrain-merror:0.325768\ttest-merror:0.413297\n",
      "[150]\ttrain-merror:0.31399\ttest-merror:0.416213\n",
      "[175]\ttrain-merror:0.303936\ttest-merror:0.419323\n",
      "[200]\ttrain-merror:0.296945\ttest-merror:0.421851\n",
      "[225]\ttrain-merror:0.289668\ttest-merror:0.423212\n",
      "[250]\ttrain-merror:0.280379\ttest-merror:0.426905\n",
      "[275]\ttrain-merror:0.271953\ttest-merror:0.426905\n",
      "[300]\ttrain-merror:0.264771\ttest-merror:0.42846\n",
      "[325]\ttrain-merror:0.257972\ttest-merror:0.429627\n",
      "[350]\ttrain-merror:0.251556\ttest-merror:0.43021\n",
      "[375]\ttrain-merror:0.244278\ttest-merror:0.431571\n",
      "[400]\ttrain-merror:0.238341\ttest-merror:0.431765\n",
      "[425]\ttrain-merror:0.230681\ttest-merror:0.431571\n",
      "[450]\ttrain-merror:0.225989\ttest-merror:0.432737\n",
      "[475]\ttrain-merror:0.220435\ttest-merror:0.433904\n",
      "CPU times: user 2min 50s, sys: 10.6 s, total: 3min\n",
      "Wall time: 58.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param = {'silent':1, 'objective':'multi:softmax' }\n",
    "param['nthread'] = 8\n",
    "param['eval_metric'] = 'merror'\n",
    "param['eta'] = 0.075\n",
    "param['max_depth'] = 5\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 7\n",
    "param['n_estimators'] = 300\n",
    "param['early_stopping_rounds'] = 30\n",
    "param['learning_rates'] = 0.2\n",
    "\n",
    "watchlist = [ (dtrain,'train'), (dtest, 'test') ]\n",
    "\n",
    "bst = xgb.train(param, dtrain, 500, watchlist, verbose_eval=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
